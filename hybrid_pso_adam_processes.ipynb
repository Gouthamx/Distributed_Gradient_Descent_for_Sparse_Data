{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hybrid PSO + Adam with Multiprocessing Nodes\n",
        "\n",
        "This notebook implements a **hybrid PSO–Adam training setup** using **processes as nodes**.\n",
        "\n",
        "- We reuse the **data splits from the previous data-split approach** via `splits.pt` created in `Approach_1.ipynb`.\n",
        "- We create **5 worker processes (PSO/Adam nodes)** and **1 main node (Adam + aggregation)**.\n",
        "- Communication between nodes uses **`multiprocessing.Queue`** to minimize IPC operations.\n",
        "- We **track the number of queue reads/writes** and compare communication cost with model performance.\n",
        "\n",
        "High-level loop per communication round:\n",
        "\n",
        "1. Main node sends the current global model to each worker.\n",
        "2. Each worker trains locally (Adam + PSO-inspired search) for a few epochs and returns its best model.\n",
        "3. Main node **aggregates the 5 worker models** using the **same weighted-averaging aggregator** used before.\n",
        "4. Main node runs **Adam on the aggregated model** for a few epochs.\n",
        "5. Steps 1–4 repeat for several rounds while we log metrics and IPC counts."
      ],
      "id": "0ca64c8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import math\n",
        "import queue\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from multiprocessing import Process, Queue, set_start_method\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For Windows / notebook safety\n",
        "try:\n",
        "    set_start_method(\"spawn\")\n",
        "except RuntimeError:\n",
        "    # Already set in this interpreter\n",
        "    pass\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print(\"Using device:\", DEVICE)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3ba019be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load tensor splits created in Approach_1.ipynb\n",
        "\n",
        "splits_path = \"splits.pt\"\n",
        "assert os.path.exists(splits_path), f\"{splits_path} not found. Please run the data-split notebook first.\"\n",
        "\n",
        "raw_splits = torch.load(splits_path)\n",
        "print(f\"Loaded {len(raw_splits)} tensor splits from {splits_path}\")\n",
        "\n",
        "\n",
        "def make_dataloaders_from_splits(tensor_splits, batch_size=1024):\n",
        "    \"\"\"Create train/test DataLoaders for each split.\"\"\"\n",
        "    loaders = []\n",
        "    for i, s in enumerate(tensor_splits):\n",
        "        X_train, y_train = s[\"X_train\"], s[\"y_train\"]\n",
        "        X_test, y_test = s[\"X_test\"], s[\"y_test\"]\n",
        "\n",
        "        train_ds = TensorDataset(X_train, y_train)\n",
        "        test_ds = TensorDataset(X_test, y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        loaders.append({\n",
        "            \"train_loader\": train_loader,\n",
        "            \"test_loader\": test_loader,\n",
        "        })\n",
        "\n",
        "    return loaders\n",
        "\n",
        "split_loaders = make_dataloaders_from_splits(raw_splits, batch_size=2048)\n",
        "print(f\"Created DataLoaders for {len(split_loaders)} splits\")\n",
        "\n",
        "# Infer global user/movie ID ranges from the first split\n",
        "example_X = raw_splits[0][\"X_train\"]\n",
        "n_users_global = int(example_X[:, 0].max().item()) + 1\n",
        "n_movies_global = int(example_X[:, 1].max().item()) + 1\n",
        "\n",
        "print(\"n_users_global =\", n_users_global)\n",
        "print(\"n_movies_global =\", n_movies_global)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b5aceb7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model definition (same as in Approach_1)\n",
        "\n",
        "class CollabFiltering(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, emb_dim=16, hidden=16, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
        "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
        "        self.dropout_emb = 0.4\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim * 2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        u = F.dropout(self.user_emb(user), p=self.dropout_emb, training=self.training)\n",
        "        m = F.dropout(self.movie_emb(movie), p=self.dropout_emb, training=self.training)\n",
        "        x = torch.cat([u, m], dim=1)\n",
        "        return self.mlp(x).squeeze()\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = CollabFiltering(n_users_global, n_movies_global, emb_dim=16, hidden=16, dropout=0.1)\n",
        "    return model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "00d12e31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model definition (same as in Approach_1)\n",
        "\n",
        "class CollabFiltering(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, emb_dim=16, hidden=16, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
        "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
        "        self.dropout_emb = 0.4\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim * 2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        u = F.dropout(self.user_emb(user), p=self.dropout_emb, training=self.training)\n",
        "        m = F.dropout(self.movie_emb(movie), p=self.dropout_emb, training=self.training)\n",
        "        x = torch.cat([u, m], dim=1)\n",
        "        return self.mlp(x).squeeze()\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = CollabFiltering(n_users_global, n_movies_global, emb_dim=16, hidden=16, dropout=0.1)\n",
        "    return model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4fdc5efd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aggregation utilities (same technique as before: weighted average of state_dicts)\n",
        "\n",
        "@torch.no_grad()\n",
        "def aggregate_models_cpu(weights, node_states):\n",
        "    \"\"\"Weighted average of multiple model state_dicts on CPU.\"\"\"\n",
        "    n_nodes = len(node_states)\n",
        "    assert len(weights) == n_nodes\n",
        "\n",
        "    agg_state = {}\n",
        "    for key in node_states[0].keys():\n",
        "        agg_param = torch.zeros_like(node_states[0][key])\n",
        "        for i in range(n_nodes):\n",
        "            agg_param += weights[i] * node_states[i][key]\n",
        "        agg_state[key] = agg_param\n",
        "    return agg_state\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model_state(state_dict, model_template, data_loader, loss_fn):\n",
        "    model = copy.deepcopy(model_template)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    for X_batch, y_batch in data_loader:\n",
        "        X_batch = X_batch.to(DEVICE)\n",
        "        y_batch = y_batch.float().to(DEVICE)\n",
        "        preds = model(X_batch[:, 0].long(), X_batch[:, 1].long())\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "    return total_loss / max(total_batches, 1)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2e6e5e1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Local training helpers\n",
        "\n",
        "\n",
        "def train_one_epoch_adam(model, train_loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch = X_batch.to(DEVICE)\n",
        "        y_batch = y_batch.float().to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_batch[:, 0].long(), X_batch[:, 1].long())\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "    return total_loss / max(total_batches, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, data_loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    for X_batch, y_batch in data_loader:\n",
        "        X_batch = X_batch.to(DEVICE)\n",
        "        y_batch = y_batch.float().to(DEVICE)\n",
        "        preds = model(X_batch[:, 0].long(), X_batch[:, 1].long())\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "    return total_loss / max(total_batches, 1)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cdc614ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Worker process: represents a PSO/Adam node\n",
        "\n",
        "\n",
        "def worker_node(worker_id, cmd_queue, result_queue, split_index,\n",
        "                base_lr=1e-3, local_epochs=5):\n",
        "    \"\"\"Worker loop running in a separate process.\n",
        "\n",
        "    Each worker:\n",
        "    - Receives the current global model parameters.\n",
        "    - Trains locally with Adam for a few epochs on its data split.\n",
        "    - Sends back its best local model and metrics.\n",
        "    \"\"\"\n",
        "    # Recreate model and data inside the process (safe under spawn)\n",
        "    model = create_model()\n",
        "    loaders = split_loaders[split_index]\n",
        "    train_loader = loaders[\"train_loader\"]\n",
        "    test_loader = loaders[\"test_loader\"]\n",
        "\n",
        "    while True:\n",
        "        msg = cmd_queue.get()  # 1 read on cmd_queue\n",
        "        msg_type = msg.get(\"type\", None)\n",
        "\n",
        "        if msg_type == \"stop\":\n",
        "            break\n",
        "\n",
        "        assert msg_type == \"train\", f\"Unknown message type: {msg_type}\"\n",
        "        round_idx = msg[\"round\"]\n",
        "        global_state = msg[\"state_dict\"]\n",
        "\n",
        "        # Load global model\n",
        "        model.load_state_dict(global_state)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
        "\n",
        "        # Local training (Adam)\n",
        "        last_train_loss = None\n",
        "        for _ in range(local_epochs):\n",
        "            last_train_loss = train_one_epoch_adam(model, train_loader, optimizer, loss_fn)\n",
        "\n",
        "        # Evaluate on local test data\n",
        "        local_test_loss = evaluate_model(model, test_loader, loss_fn)\n",
        "\n",
        "        # Send result back to main node (1 write on result_queue)\n",
        "        result_queue.put({\n",
        "            \"worker_id\": worker_id,\n",
        "            \"round\": round_idx,\n",
        "            \"state_dict\": copy.deepcopy(model.state_dict()),\n",
        "            \"train_loss\": float(last_train_loss),\n",
        "            \"test_loss\": float(local_test_loss),\n",
        "        })"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6dfbcdd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main orchestration: 5 worker nodes + 1 main node\n",
        "\n",
        "\n",
        "def run_hybrid_pso_adam(num_rounds=3,\n",
        "                         num_workers=5,\n",
        "                         local_epochs=5,\n",
        "                         global_adam_epochs=3,\n",
        "                         base_lr=1e-3):\n",
        "    assert num_workers <= len(split_loaders)\n",
        "\n",
        "    # Queues for IPC\n",
        "    cmd_queues = [Queue() for _ in range(num_workers)]  # per-worker command queue\n",
        "    result_queue = Queue()  # shared results queue\n",
        "\n",
        "    # Spawn worker processes\n",
        "    workers = []\n",
        "    for wid in range(num_workers):\n",
        "        p = Process(\n",
        "            target=worker_node,\n",
        "            args=(wid, cmd_queues[wid], result_queue, wid, base_lr, local_epochs),\n",
        "        )\n",
        "        p.start()\n",
        "        workers.append(p)\n",
        "\n",
        "    # Main node model\n",
        "    global_model = create_model()\n",
        "\n",
        "    # Use the first split's test loader as a simple global validation set\n",
        "    val_loader = split_loaders[0][\"test_loader\"]\n",
        "\n",
        "    history = {\n",
        "        \"round\": [],\n",
        "        \"global_val_loss\": [],\n",
        "        \"avg_worker_test_loss\": [],\n",
        "    }\n",
        "\n",
        "    reads_per_round = []\n",
        "    writes_per_round = []\n",
        "\n",
        "    for r in range(num_rounds):\n",
        "        # 1) Main node broadcasts current global model to all workers\n",
        "        state_to_send = copy.deepcopy(global_model.state_dict())\n",
        "        for q in cmd_queues:\n",
        "            q.put({\"type\": \"train\", \"round\": r, \"state_dict\": state_to_send})\n",
        "        writes_this_round = num_workers  # command writes\n",
        "\n",
        "        # 2) Collect results from workers\n",
        "        worker_states = []\n",
        "        worker_test_losses = []\n",
        "        for _ in range(num_workers):\n",
        "            msg = result_queue.get()\n",
        "            worker_states.append(msg[\"state_dict\"])\n",
        "            worker_test_losses.append(msg[\"test_loss\"])\n",
        "        reads_this_round = num_workers  # result reads\n",
        "\n",
        "        # 3) PSO aggregation on the 5 worker models (hybrid PSO + Adam)\n",
        "        model_template = create_model()\n",
        "        best_weights, best_score = pso_optimize_aggregation(\n",
        "            worker_states,\n",
        "            val_loader,\n",
        "            model_template,\n",
        "            loss_fn,\n",
        "            num_particles=10,\n",
        "            max_iters=5,\n",
        "        )\n",
        "        agg_state = aggregate_models_cpu(best_weights, worker_states)\n",
        "\n",
        "        # 4) Main node runs Adam on aggregated model for a few epochs\n",
        "        global_model.load_state_dict(agg_state)\n",
        "        optimizer = torch.optim.Adam(global_model.parameters(), lr=base_lr)\n",
        "        for _ in range(global_adam_epochs):\n",
        "            _ = train_one_epoch_adam(global_model, val_loader, optimizer, loss_fn)\n",
        "\n",
        "        # 5) Log metrics\n",
        "        global_val_loss = evaluate_model(global_model, val_loader, loss_fn)\n",
        "        history[\"round\"].append(r)\n",
        "        history[\"global_val_loss\"].append(float(global_val_loss))\n",
        "        history[\"avg_worker_test_loss\"].append(float(np.mean(worker_test_losses)))\n",
        "\n",
        "        reads_per_round.append(reads_this_round)\n",
        "        writes_per_round.append(writes_this_round)\n",
        "\n",
        "        print(\n",
        "            f\"Round {r}: global_val_loss={global_val_loss:.4f}, \"\n",
        "            f\"avg_worker_test_loss={np.mean(worker_test_losses):.4f}\"\n",
        "        )\n",
        "\n",
        "    # 6) Stop workers (one extra command per worker)\n",
        "    for q in cmd_queues:\n",
        "        q.put({\"type\": \"stop\"})\n",
        "    extra_writes = num_workers\n",
        "\n",
        "    for p in workers:\n",
        "        p.join()\n",
        "\n",
        "    # Analytical IPC accounting (Queue-based, each message = 1 write + 1 read)\n",
        "    train_messages = num_rounds * (2 * num_workers)  # train commands + results\n",
        "    stop_messages = num_workers  # stop commands\n",
        "    total_messages = train_messages + stop_messages\n",
        "\n",
        "    comm_stats = {\n",
        "        \"num_rounds\": num_rounds,\n",
        "        \"num_workers\": num_workers,\n",
        "        \"reads_per_round\": reads_per_round,\n",
        "        \"writes_per_round\": writes_per_round,\n",
        "        \"total_train_round_messages\": train_messages,\n",
        "        \"total_stop_messages\": stop_messages,\n",
        "        \"total_queue_writes\": total_messages,\n",
        "        \"total_queue_reads\": total_messages,\n",
        "    }\n",
        "\n",
        "    return global_model, history, comm_stats"
      ],
      "id": "d0bd4330",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main orchestration: 5 worker nodes + 1 main node\n",
        "\n",
        "\n",
        "def run_hybrid_pso_adam(num_rounds=3,\n",
        "                         num_workers=5,\n",
        "                         local_epochs=5,\n",
        "                         global_adam_epochs=3,\n",
        "                         base_lr=1e-3):\n",
        "    assert num_workers <= len(split_loaders)\n",
        "\n",
        "    # Queues for IPC\n",
        "    cmd_queues = [Queue() for _ in range(num_workers)]  # per-worker command queue\n",
        "    result_queue = Queue()  # shared results queue\n",
        "\n",
        "    # Spawn worker processes\n",
        "    workers = []\n",
        "    for wid in range(num_workers):\n",
        "        p = Process(\n",
        "            target=worker_node,\n",
        "            args=(wid, cmd_queues[wid], result_queue, wid, base_lr, local_epochs),\n",
        "        )\n",
        "        p.start()\n",
        "        workers.append(p)\n",
        "\n",
        "    # Main node model\n",
        "    global_model = create_model()\n",
        "\n",
        "    # Use the first split's test loader as a simple global validation set\n",
        "    val_loader = split_loaders[0][\"test_loader\"]\n",
        "\n",
        "    history = {\n",
        "        \"round\": [],\n",
        "        \"global_val_loss\": [],\n",
        "        \"avg_worker_test_loss\": [],\n",
        "    }\n",
        "\n",
        "    reads_per_round = []\n",
        "    writes_per_round = []\n",
        "\n",
        "    for r in range(num_rounds):\n",
        "        # 1) Main node broadcasts current global model to all workers\n",
        "        state_to_send = copy.deepcopy(global_model.state_dict())\n",
        "        for q in cmd_queues:\n",
        "            q.put({\"type\": \"train\", \"round\": r, \"state_dict\": state_to_send})\n",
        "        writes_this_round = num_workers  # command writes\n",
        "\n",
        "        # 2) Collect results from workers\n",
        "        worker_states = []\n",
        "        worker_test_losses = []\n",
        "        for _ in range(num_workers):\n",
        "            msg = result_queue.get()\n",
        "            worker_states.append(msg[\"state_dict\"])\n",
        "            worker_test_losses.append(msg[\"test_loss\"])\n",
        "        reads_this_round = num_workers  # result reads\n",
        "\n",
        "        # 3) PSO aggregation on the 5 worker models (hybrid PSO + Adam)\n",
        "        model_template = create_model()\n",
        "        best_weights, best_score = pso_optimize_aggregation(\n",
        "            worker_states,\n",
        "            val_loader,\n",
        "            model_template,\n",
        "            loss_fn,\n",
        "            num_particles=10,\n",
        "            max_iters=5,\n",
        "        )\n",
        "        agg_state = aggregate_models_cpu(best_weights, worker_states)\n",
        "\n",
        "        # 4) Main node runs Adam on aggregated model for a few epochs\n",
        "        global_model.load_state_dict(agg_state)\n",
        "        optimizer = torch.optim.Adam(global_model.parameters(), lr=base_lr)\n",
        "        for _ in range(global_adam_epochs):\n",
        "            _ = train_one_epoch_adam(global_model, val_loader, optimizer, loss_fn)\n",
        "\n",
        "        # 5) Log metrics\n",
        "        global_val_loss = evaluate_model(global_model, val_loader, loss_fn)\n",
        "        history[\"round\"].append(r)\n",
        "        history[\"global_val_loss\"].append(float(global_val_loss))\n",
        "        history[\"avg_worker_test_loss\"].append(float(np.mean(worker_test_losses)))\n",
        "\n",
        "        reads_per_round.append(reads_this_round)\n",
        "        writes_per_round.append(writes_this_round)\n",
        "\n",
        "        print(\n",
        "            f\"Round {r}: global_val_loss={global_val_loss:.4f}, \"\n",
        "            f\"avg_worker_test_loss={np.mean(worker_test_losses):.4f}\"\n",
        "        )\n",
        "\n",
        "    # 6) Stop workers (one extra command per worker)\n",
        "    for q in cmd_queues:\n",
        "        q.put({\"type\": \"stop\"})\n",
        "    extra_writes = num_workers\n",
        "\n",
        "    for p in workers:\n",
        "        p.join()\n",
        "\n",
        "    # Analytical IPC accounting (Queue-based, each message = 1 write + 1 read)\n",
        "    train_messages = num_rounds * (2 * num_workers)  # train commands + results\n",
        "    stop_messages = num_workers  # stop commands\n",
        "    total_messages = train_messages + stop_messages\n",
        "\n",
        "    comm_stats = {\n",
        "        \"num_rounds\": num_rounds,\n",
        "        \"num_workers\": num_workers,\n",
        "        \"reads_per_round\": reads_per_round,\n",
        "        \"writes_per_round\": writes_per_round,\n",
        "        \"total_train_round_messages\": train_messages,\n",
        "        \"total_stop_messages\": stop_messages,\n",
        "        \"total_queue_writes\": total_messages,\n",
        "        \"total_queue_reads\": total_messages,\n",
        "    }\n",
        "\n",
        "    return global_model, history, comm_stats"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ff2ee4e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run the hybrid PSO–Adam experiment\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    global_model, history, comm_stats = run_hybrid_pso_adam(\n",
        "        num_rounds=3,\n",
        "        num_workers=5,\n",
        "        local_epochs=5,\n",
        "        global_adam_epochs=3,\n",
        "        base_lr=1e-3,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Communication statistics (Queue-based IPC) ===\")\n",
        "    for k, v in comm_stats.items():\n",
        "        print(f\"{k}: {v}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fccd9c85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot losses and IPC counts\n",
        "\n",
        "rounds = history[\"round\"]\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(rounds, history[\"global_val_loss\"], marker=\"o\", label=\"Global (Adam after aggregation)\")\n",
        "plt.plot(rounds, history[\"avg_worker_test_loss\"], marker=\"s\", label=\"Avg worker test loss\")\n",
        "plt.xlabel(\"Communication round\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.title(\"Hybrid PSO–Adam: Loss vs Rounds\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# IPC counts per round\n",
        "plt.subplot(1, 2, 2)\n",
        "reads = comm_stats[\"reads_per_round\"]\n",
        "writes = comm_stats[\"writes_per_round\"]\n",
        "indices = np.arange(len(rounds))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(indices - width / 2, writes, width, label=\"Writes (commands)\")\n",
        "plt.bar(indices + width / 2, reads, width, label=\"Reads (results)\")\n",
        "plt.xticks(indices, rounds)\n",
        "plt.xlabel(\"Communication round\")\n",
        "plt.ylabel(\"Queue ops per round\")\n",
        "plt.title(\"IPC: Queue Reads/Writes per Round\")\n",
        "plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTotal queue writes:\", comm_stats[\"total_queue_writes\"])\n",
        "print(\"Total queue reads:\", comm_stats[\"total_queue_reads\"])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0f1a9439"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple PSO over aggregation weights (reusing the idea from Approach_1)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def pso_optimize_aggregation(node_states, val_loader, model_template, loss_fn,\n",
        "                             num_particles=10, max_iters=5, w=0.7, c1=1.5, c2=1.5):\n",
        "    \"\"\"Small PSO to find good aggregation weights for the 5 workers.\"\"\"\n",
        "    num_nodes = len(node_states)\n",
        "\n",
        "    # Initialize particles on the simplex via Dirichlet\n",
        "    particles = np.random.dirichlet(np.ones(num_nodes), size=num_particles)\n",
        "    velocities = np.zeros_like(particles)\n",
        "    pbest_positions = particles.copy()\n",
        "    pbest_scores = np.full(num_particles, np.inf)\n",
        "\n",
        "    gbest_position = None\n",
        "    gbest_score = np.inf\n",
        "\n",
        "    for it in range(max_iters):\n",
        "        for i in range(num_particles):\n",
        "            w_vec = np.abs(particles[i])\n",
        "            w_vec /= np.sum(w_vec)\n",
        "\n",
        "            agg_state = aggregate_models_cpu(w_vec, node_states)\n",
        "            score = evaluate_model_state(agg_state, model_template, val_loader, loss_fn)\n",
        "\n",
        "            if score < pbest_scores[i]:\n",
        "                pbest_scores[i] = score\n",
        "                pbest_positions[i] = w_vec.copy()\n",
        "\n",
        "            if score < gbest_score:\n",
        "                gbest_score = score\n",
        "                gbest_position = w_vec.copy()\n",
        "\n",
        "        # Velocity/position update\n",
        "        for i in range(num_particles):\n",
        "            r1 = np.random.rand(num_nodes)\n",
        "            r2 = np.random.rand(num_nodes)\n",
        "            velocities[i] = (\n",
        "                w * velocities[i]\n",
        "                + c1 * r1 * (pbest_positions[i] - particles[i])\n",
        "                + c2 * r2 * (gbest_position - particles[i])\n",
        "            )\n",
        "            particles[i] += velocities[i]\n",
        "\n",
        "    return gbest_position, gbest_score"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a15b68a1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}