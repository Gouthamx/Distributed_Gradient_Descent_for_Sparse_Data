{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hybrid PSO + Adam with Multiprocessing Nodes\n",
        "\n",
        "This notebook implements a **hybrid PSO–Adam training setup** using **processes as nodes**.\n",
        "\n",
        "- We reuse the **data splits from the previous data-split approach** via `splits.pt` created in `Approach_1.ipynb`.\n",
        "- We create **5 worker processes (PSO/Adam nodes)** and **1 main node (Adam + aggregation)**.\n",
        "- Communication between nodes uses **`multiprocessing.Queue`** to minimize IPC operations.\n",
        "- We **track the number of queue reads/writes** and compare communication cost with model performance.\n",
        "\n",
        "High-level loop per communication round:\n",
        "\n",
        "1. Main node sends the current global model to each worker.\n",
        "2. Each worker trains locally (Adam + PSO-inspired search) for a few epochs and returns its best model.\n",
        "3. Main node **aggregates the 5 worker models** using the **same weighted-averaging aggregator** used before.\n",
        "4. Main node runs **Adam on the aggregated model** for a few epochs.\n",
        "5. Steps 1–4 repeat for several rounds while we log metrics and IPC counts."
      ],
      "id": "0ca64c8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import math\n",
        "import queue\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from multiprocessing import Process, Queue, set_start_method\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For Windows / notebook safety\n",
        "try:\n",
        "    set_start_method(\"spawn\")\n",
        "except RuntimeError:\n",
        "    # Already set in this interpreter\n",
        "    pass\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print(\"Using device:\", DEVICE)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "id": "3ba019be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load tensor splits created in Approach_1.ipynb\n",
        "\n",
        "splits_path = \"splits.pt\"\n",
        "assert os.path.exists(splits_path), f\"{splits_path} not found. Please run the data-split notebook first.\"\n",
        "\n",
        "raw_splits = torch.load(splits_path)\n",
        "print(f\"Loaded {len(raw_splits)} tensor splits from {splits_path}\")\n",
        "\n",
        "\n",
        "def make_dataloaders_from_splits(tensor_splits, batch_size=1024):\n",
        "    \"\"\"Create train/test DataLoaders for each split.\"\"\"\n",
        "    loaders = []\n",
        "    for i, s in enumerate(tensor_splits):\n",
        "        X_train, y_train = s[\"X_train\"], s[\"y_train\"]\n",
        "        X_test, y_test = s[\"X_test\"], s[\"y_test\"]\n",
        "\n",
        "        train_ds = TensorDataset(X_train, y_train)\n",
        "        test_ds = TensorDataset(X_test, y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        loaders.append({\n",
        "            \"train_loader\": train_loader,\n",
        "            \"test_loader\": test_loader,\n",
        "        })\n",
        "\n",
        "    return loaders\n",
        "\n",
        "split_loaders = make_dataloaders_from_splits(raw_splits, batch_size=2048)\n",
        "print(f\"Created DataLoaders for {len(split_loaders)} splits\")\n",
        "\n",
        "# Infer global user/movie ID ranges from the first split\n",
        "example_X = raw_splits[0][\"X_train\"]\n",
        "n_users_global = int(example_X[:, 0].max().item()) + 1\n",
        "n_movies_global = int(example_X[:, 1].max().item()) + 1\n",
        "\n",
        "print(\"n_users_global =\", n_users_global)\n",
        "print(\"n_movies_global =\", n_movies_global)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 5 tensor splits from splits.pt\n",
            "Created DataLoaders for 5 splits\n",
            "n_users_global = 1000\n",
            "n_movies_global = 11453\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\gouth\\AppData\\Local\\Temp\\ipykernel_16452\\2220191918.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  raw_splits = torch.load(splits_path)\n"
          ]
        }
      ],
      "id": "b5aceb7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model definition (same as in Approach_1)\n",
        "\n",
        "class CollabFiltering(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, emb_dim=16, hidden=16, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
        "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
        "        self.dropout_emb = 0.4\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim * 2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        u = F.dropout(self.user_emb(user), p=self.dropout_emb, training=self.training)\n",
        "        m = F.dropout(self.movie_emb(movie), p=self.dropout_emb, training=self.training)\n",
        "        x = torch.cat([u, m], dim=1)\n",
        "        return self.mlp(x).squeeze()\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = CollabFiltering(n_users_global, n_movies_global, emb_dim=16, hidden=16, dropout=0.1)\n",
        "    return model.to(DEVICE)"
      ],
      "execution_count": 3,
      "outputs": [],
      "id": "00d12e31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model definition (same as in Approach_1)\n",
        "\n",
        "class CollabFiltering(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, emb_dim=16, hidden=16, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
        "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
        "        self.dropout_emb = 0.4\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim * 2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        u = F.dropout(self.user_emb(user), p=self.dropout_emb, training=self.training)\n",
        "        m = F.dropout(self.movie_emb(movie), p=self.dropout_emb, training=self.training)\n",
        "        x = torch.cat([u, m], dim=1)\n",
        "        return self.mlp(x).squeeze()\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = CollabFiltering(n_users_global, n_movies_global, emb_dim=16, hidden=16, dropout=0.1)\n",
        "    return model.to(DEVICE)"
      ],
      "execution_count": 4,
      "outputs": [],
      "id": "4fdc5efd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Aggregation utilities (same technique as before: weighted average of state_dicts)\n",
        "\n",
        "@torch.no_grad()\n",
        "def aggregate_models_cpu(weights, node_states):\n",
        "    \"\"\"Weighted average of multiple model state_dicts on CPU.\"\"\"\n",
        "    n_nodes = len(node_states)\n",
        "    assert len(weights) == n_nodes\n",
        "\n",
        "    agg_state = {}\n",
        "    for key in node_states[0].keys():\n",
        "        agg_param = torch.zeros_like(node_states[0][key])\n",
        "        for i in range(n_nodes):\n",
        "            agg_param += weights[i] * node_states[i][key]\n",
        "        agg_state[key] = agg_param\n",
        "    return agg_state\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model_state(state_dict, model_template, data_loader, loss_fn):\n",
        "    model = copy.deepcopy(model_template)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    for X_batch, y_batch in data_loader:\n",
        "        X_batch = X_batch.to(DEVICE)\n",
        "        y_batch = y_batch.float().to(DEVICE)\n",
        "        preds = model(X_batch[:, 0].long(), X_batch[:, 1].long())\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "    return total_loss / max(total_batches, 1)"
      ],
      "execution_count": 5,
      "outputs": [],
      "id": "2e6e5e1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Local training helpers\n",
        "\n",
        "\n",
        "def train_one_epoch_adam(model, train_loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch = X_batch.to(DEVICE)\n",
        "        y_batch = y_batch.float().to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_batch[:, 0].long(), X_batch[:, 1].long())\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "    return total_loss / max(total_batches, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, data_loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "    for X_batch, y_batch in data_loader:\n",
        "        X_batch = X_batch.to(DEVICE)\n",
        "        y_batch = y_batch.float().to(DEVICE)\n",
        "        preds = model(X_batch[:, 0].long(), X_batch[:, 1].long())\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "    return total_loss / max(total_batches, 1)"
      ],
      "execution_count": 6,
      "outputs": [],
      "id": "cdc614ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Worker process: represents a PSO/Adam node\n",
        "\n",
        "\n",
        "def worker_node(worker_id, cmd_queue, result_queue, split_index,\n",
        "                base_lr=1e-3, local_epochs=5):\n",
        "    \"\"\"Worker loop running in a separate process.\n",
        "\n",
        "    Each worker:\n",
        "    - Receives the current global model parameters.\n",
        "    - Trains locally with Adam for a few epochs on its data split.\n",
        "    - Sends back its best local model and metrics.\n",
        "    \"\"\"\n",
        "    # Recreate model and data inside the process (safe under spawn)\n",
        "    model = create_model()\n",
        "    loaders = split_loaders[split_index]\n",
        "    train_loader = loaders[\"train_loader\"]\n",
        "    test_loader = loaders[\"test_loader\"]\n",
        "\n",
        "    while True:\n",
        "        msg = cmd_queue.get()  # 1 read on cmd_queue\n",
        "        msg_type = msg.get(\"type\", None)\n",
        "\n",
        "        if msg_type == \"stop\":\n",
        "            break\n",
        "\n",
        "        assert msg_type == \"train\", f\"Unknown message type: {msg_type}\"\n",
        "        round_idx = msg[\"round\"]\n",
        "        global_state = msg[\"state_dict\"]\n",
        "\n",
        "        # Load global model\n",
        "        model.load_state_dict(global_state)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
        "\n",
        "        # Local training (Adam)\n",
        "        last_train_loss = None\n",
        "        for _ in range(local_epochs):\n",
        "            last_train_loss = train_one_epoch_adam(model, train_loader, optimizer, loss_fn)\n",
        "\n",
        "        # Evaluate on local test data\n",
        "        local_test_loss = evaluate_model(model, test_loader, loss_fn)\n",
        "\n",
        "        # Send result back to main node (1 write on result_queue)\n",
        "        result_queue.put({\n",
        "            \"worker_id\": worker_id,\n",
        "            \"round\": round_idx,\n",
        "            \"state_dict\": copy.deepcopy(model.state_dict()),\n",
        "            \"train_loss\": float(last_train_loss),\n",
        "            \"test_loss\": float(local_test_loss),\n",
        "        })"
      ],
      "execution_count": 7,
      "outputs": [],
      "id": "6dfbcdd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main orchestration: 5 worker nodes + 1 main node\n",
        "\n",
        "\n",
        "def run_hybrid_pso_adam(num_rounds=3,\n",
        "                         num_workers=5,\n",
        "                         local_epochs=5,\n",
        "                         global_adam_epochs=3,\n",
        "                         base_lr=1e-3):\n",
        "    assert num_workers <= len(split_loaders)\n",
        "\n",
        "    # Queues for IPC\n",
        "    cmd_queues = [Queue() for _ in range(num_workers)]  # per-worker command queue\n",
        "    result_queue = Queue()  # shared results queue\n",
        "\n",
        "    # Spawn worker processes\n",
        "    workers = []\n",
        "    for wid in range(num_workers):\n",
        "        p = Process(\n",
        "            target=worker_node,\n",
        "            args=(wid, cmd_queues[wid], result_queue, wid, base_lr, local_epochs),\n",
        "        )\n",
        "        p.start()\n",
        "        workers.append(p)\n",
        "\n",
        "    # Main node model\n",
        "    global_model = create_model()\n",
        "\n",
        "    # Use the first split's test loader as a simple global validation set\n",
        "    val_loader = split_loaders[0][\"test_loader\"]\n",
        "\n",
        "    history = {\n",
        "        \"round\": [],\n",
        "        \"global_val_loss\": [],\n",
        "        \"avg_worker_test_loss\": [],\n",
        "    }\n",
        "\n",
        "    reads_per_round = []\n",
        "    writes_per_round = []\n",
        "\n",
        "    for r in range(num_rounds):\n",
        "        # 1) Main node broadcasts current global model to all workers\n",
        "        state_to_send = copy.deepcopy(global_model.state_dict())\n",
        "        for q in cmd_queues:\n",
        "            q.put({\"type\": \"train\", \"round\": r, \"state_dict\": state_to_send})\n",
        "        writes_this_round = num_workers  # command writes\n",
        "\n",
        "        # 2) Collect results from workers\n",
        "        worker_states = []\n",
        "        worker_test_losses = []\n",
        "        for _ in range(num_workers):\n",
        "            msg = result_queue.get()\n",
        "            worker_states.append(msg[\"state_dict\"])\n",
        "            worker_test_losses.append(msg[\"test_loss\"])\n",
        "        reads_this_round = num_workers  # result reads\n",
        "\n",
        "        # 3) PSO aggregation on the 5 worker models (hybrid PSO + Adam)\n",
        "        model_template = create_model()\n",
        "        best_weights, best_score = pso_optimize_aggregation(\n",
        "            worker_states,\n",
        "            val_loader,\n",
        "            model_template,\n",
        "            loss_fn,\n",
        "            num_particles=10,\n",
        "            max_iters=5,\n",
        "        )\n",
        "        agg_state = aggregate_models_cpu(best_weights, worker_states)\n",
        "\n",
        "        # 4) Main node runs Adam on aggregated model for a few epochs\n",
        "        global_model.load_state_dict(agg_state)\n",
        "        optimizer = torch.optim.Adam(global_model.parameters(), lr=base_lr)\n",
        "        for _ in range(global_adam_epochs):\n",
        "            _ = train_one_epoch_adam(global_model, val_loader, optimizer, loss_fn)\n",
        "\n",
        "        # 5) Log metrics\n",
        "        global_val_loss = evaluate_model(global_model, val_loader, loss_fn)\n",
        "        history[\"round\"].append(r)\n",
        "        history[\"global_val_loss\"].append(float(global_val_loss))\n",
        "        history[\"avg_worker_test_loss\"].append(float(np.mean(worker_test_losses)))\n",
        "\n",
        "        reads_per_round.append(reads_this_round)\n",
        "        writes_per_round.append(writes_this_round)\n",
        "\n",
        "        print(\n",
        "            f\"Round {r}: global_val_loss={global_val_loss:.4f}, \"\n",
        "            f\"avg_worker_test_loss={np.mean(worker_test_losses):.4f}\"\n",
        "        )\n",
        "\n",
        "    # 6) Stop workers (one extra command per worker)\n",
        "    for q in cmd_queues:\n",
        "        q.put({\"type\": \"stop\"})\n",
        "    extra_writes = num_workers\n",
        "\n",
        "    for p in workers:\n",
        "        p.join()\n",
        "\n",
        "    # Analytical IPC accounting (Queue-based, each message = 1 write + 1 read)\n",
        "    train_messages = num_rounds * (2 * num_workers)  # train commands + results\n",
        "    stop_messages = num_workers  # stop commands\n",
        "    total_messages = train_messages + stop_messages\n",
        "\n",
        "    comm_stats = {\n",
        "        \"num_rounds\": num_rounds,\n",
        "        \"num_workers\": num_workers,\n",
        "        \"reads_per_round\": reads_per_round,\n",
        "        \"writes_per_round\": writes_per_round,\n",
        "        \"total_train_round_messages\": train_messages,\n",
        "        \"total_stop_messages\": stop_messages,\n",
        "        \"total_queue_writes\": total_messages,\n",
        "        \"total_queue_reads\": total_messages,\n",
        "    }\n",
        "\n",
        "    return global_model, history, comm_stats"
      ],
      "execution_count": 8,
      "outputs": [],
      "id": "d0bd4330"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main orchestration: 5 worker nodes + 1 main node\n",
        "\n",
        "\n",
        "def run_hybrid_pso_adam(num_rounds=3,\n",
        "                         num_workers=5,\n",
        "                         local_epochs=5,\n",
        "                         global_adam_epochs=3,\n",
        "                         base_lr=1e-3):\n",
        "    assert num_workers <= len(split_loaders)\n",
        "\n",
        "    # Queues for IPC\n",
        "    cmd_queues = [Queue() for _ in range(num_workers)]  # per-worker command queue\n",
        "    result_queue = Queue()  # shared results queue\n",
        "\n",
        "    # Spawn worker processes\n",
        "    workers = []\n",
        "    for wid in range(num_workers):\n",
        "        p = Process(\n",
        "            target=worker_node,\n",
        "            args=(wid, cmd_queues[wid], result_queue, wid, base_lr, local_epochs),\n",
        "        )\n",
        "        p.start()\n",
        "        workers.append(p)\n",
        "\n",
        "    # Main node model\n",
        "    global_model = create_model()\n",
        "\n",
        "    # Use the first split's test loader as a simple global validation set\n",
        "    val_loader = split_loaders[0][\"test_loader\"]\n",
        "\n",
        "    history = {\n",
        "        \"round\": [],\n",
        "        \"global_val_loss\": [],\n",
        "        \"avg_worker_test_loss\": [],\n",
        "    }\n",
        "\n",
        "    reads_per_round = []\n",
        "    writes_per_round = []\n",
        "\n",
        "    for r in range(num_rounds):\n",
        "        # 1) Main node broadcasts current global model to all workers\n",
        "        state_to_send = copy.deepcopy(global_model.state_dict())\n",
        "        for q in cmd_queues:\n",
        "            q.put({\"type\": \"train\", \"round\": r, \"state_dict\": state_to_send})\n",
        "        writes_this_round = num_workers  # command writes\n",
        "\n",
        "        # 2) Collect results from workers\n",
        "        worker_states = []\n",
        "        worker_test_losses = []\n",
        "        for _ in range(num_workers):\n",
        "            msg = result_queue.get()\n",
        "            worker_states.append(msg[\"state_dict\"])\n",
        "            worker_test_losses.append(msg[\"test_loss\"])\n",
        "        reads_this_round = num_workers  # result reads\n",
        "\n",
        "        # 3) PSO aggregation on the 5 worker models (hybrid PSO + Adam)\n",
        "        model_template = create_model()\n",
        "        best_weights, best_score = pso_optimize_aggregation(\n",
        "            worker_states,\n",
        "            val_loader,\n",
        "            model_template,\n",
        "            loss_fn,\n",
        "            num_particles=10,\n",
        "            max_iters=5,\n",
        "        )\n",
        "        agg_state = aggregate_models_cpu(best_weights, worker_states)\n",
        "\n",
        "        # 4) Main node runs Adam on aggregated model for a few epochs\n",
        "        global_model.load_state_dict(agg_state)\n",
        "        optimizer = torch.optim.Adam(global_model.parameters(), lr=base_lr)\n",
        "        for _ in range(global_adam_epochs):\n",
        "            _ = train_one_epoch_adam(global_model, val_loader, optimizer, loss_fn)\n",
        "\n",
        "        # 5) Log metrics\n",
        "        global_val_loss = evaluate_model(global_model, val_loader, loss_fn)\n",
        "        history[\"round\"].append(r)\n",
        "        history[\"global_val_loss\"].append(float(global_val_loss))\n",
        "        history[\"avg_worker_test_loss\"].append(float(np.mean(worker_test_losses)))\n",
        "\n",
        "        reads_per_round.append(reads_this_round)\n",
        "        writes_per_round.append(writes_this_round)\n",
        "\n",
        "        print(\n",
        "            f\"Round {r}: global_val_loss={global_val_loss:.4f}, \"\n",
        "            f\"avg_worker_test_loss={np.mean(worker_test_losses):.4f}\"\n",
        "        )\n",
        "\n",
        "    # 6) Stop workers (one extra command per worker)\n",
        "    for q in cmd_queues:\n",
        "        q.put({\"type\": \"stop\"})\n",
        "    extra_writes = num_workers\n",
        "\n",
        "    for p in workers:\n",
        "        p.join()\n",
        "\n",
        "    # Analytical IPC accounting (Queue-based, each message = 1 write + 1 read)\n",
        "    train_messages = num_rounds * (2 * num_workers)  # train commands + results\n",
        "    stop_messages = num_workers  # stop commands\n",
        "    total_messages = train_messages + stop_messages\n",
        "\n",
        "    comm_stats = {\n",
        "        \"num_rounds\": num_rounds,\n",
        "        \"num_workers\": num_workers,\n",
        "        \"reads_per_round\": reads_per_round,\n",
        "        \"writes_per_round\": writes_per_round,\n",
        "        \"total_train_round_messages\": train_messages,\n",
        "        \"total_stop_messages\": stop_messages,\n",
        "        \"total_queue_writes\": total_messages,\n",
        "        \"total_queue_reads\": total_messages,\n",
        "    }\n",
        "\n",
        "    return global_model, history, comm_stats"
      ],
      "execution_count": 9,
      "outputs": [],
      "id": "ff2ee4e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run the hybrid PSO–Adam experiment\n",
        "if __name__ == \"__main__\":\n",
        "    global_model, history, comm_stats = run_hybrid_pso_adam(\n",
        "        num_rounds=3,\n",
        "        num_workers=5,\n",
        "        local_epochs=5,\n",
        "        global_adam_epochs=3,\n",
        "        base_lr=1e-3,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Communication statistics (Queue-based IPC) ===\")\n",
        "    for k, v in comm_stats.items():\n",
        "        print(f\"{k}: {v}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fhab\n"
          ]
        }
      ],
      "id": "fccd9c85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot losses and IPC counts\n",
        "\n",
        "rounds = history[\"round\"]\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(rounds, history[\"global_val_loss\"], marker=\"o\", label=\"Global (Adam after aggregation)\")\n",
        "plt.plot(rounds, history[\"avg_worker_test_loss\"], marker=\"s\", label=\"Avg worker test loss\")\n",
        "plt.xlabel(\"Communication round\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.title(\"Hybrid PSO–Adam: Loss vs Rounds\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "# IPC counts per round\n",
        "plt.subplot(1, 2, 2)\n",
        "reads = comm_stats[\"reads_per_round\"]\n",
        "writes = comm_stats[\"writes_per_round\"]\n",
        "indices = np.arange(len(rounds))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(indices - width / 2, writes, width, label=\"Writes (commands)\")\n",
        "plt.bar(indices + width / 2, reads, width, label=\"Reads (results)\")\n",
        "plt.xticks(indices, rounds)\n",
        "plt.xlabel(\"Communication round\")\n",
        "plt.ylabel(\"Queue ops per round\")\n",
        "plt.title(\"IPC: Queue Reads/Writes per Round\")\n",
        "plt.grid(True, axis=\"y\", alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTotal queue writes:\", comm_stats[\"total_queue_writes\"])\n",
        "print(\"Total queue reads:\", comm_stats[\"total_queue_reads\"])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0f1a9439"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verbose PSO over aggregation weights (with progress prints)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def pso_optimize_aggregation(node_states, val_loader, model_template, loss_fn,\n",
        "                             num_particles=10, max_iters=5, w=0.7, c1=1.5, c2=1.5):\n",
        "    \"\"\"Small PSO to find good aggregation weights for the workers.\n",
        "\n",
        "    This version prints progress every iteration so we can see that it is running.\n",
        "    \"\"\"\n",
        "    num_nodes = len(node_states)\n",
        "\n",
        "    print(f\"[PSO] Starting optimization with {num_particles} particles, {max_iters} iterations, {num_nodes} nodes\")\n",
        "\n",
        "    # Initialize particles on the simplex via Dirichlet\n",
        "    particles = np.random.dirichlet(np.ones(num_nodes), size=num_particles)\n",
        "    velocities = np.zeros_like(particles)\n",
        "    pbest_positions = particles.copy()\n",
        "    pbest_scores = np.full(num_particles, np.inf)\n",
        "\n",
        "    gbest_position = None\n",
        "    gbest_score = np.inf\n",
        "\n",
        "    for it in range(max_iters):\n",
        "        for i in range(num_particles):\n",
        "            w_vec = np.abs(particles[i])\n",
        "            w_vec /= np.sum(w_vec)\n",
        "\n",
        "            agg_state = aggregate_models_cpu(w_vec, node_states)\n",
        "            score = evaluate_model_state(agg_state, model_template, val_loader, loss_fn)\n",
        "\n",
        "            if score < pbest_scores[i]:\n",
        "                pbest_scores[i] = score\n",
        "                pbest_positions[i] = w_vec.copy()\n",
        "\n",
        "            if score < gbest_score:\n",
        "                gbest_score = score\n",
        "                gbest_position = w_vec.copy()\n",
        "\n",
        "        # Velocity/position update\n",
        "        for i in range(num_particles):\n",
        "            r1 = np.random.rand(num_nodes)\n",
        "            r2 = np.random.rand(num_nodes)\n",
        "            velocities[i] = (\n",
        "                w * velocities[i]\n",
        "                + c1 * r1 * (pbest_positions[i] - particles[i])\n",
        "                + c2 * r2 * (gbest_position - particles[i])\n",
        "            )\n",
        "            particles[i] += velocities[i]\n",
        "\n",
        "        print(f\"[PSO] Iteration {it+1}/{max_iters} | best_score={gbest_score:.6f}\")\n",
        "\n",
        "    print(\"[PSO] Finished optimization.\")\n",
        "    print(\"[PSO] Best weight vector:\", np.round(gbest_position, 4))\n",
        "    print(f\"[PSO] Best validation loss: {gbest_score:.6f}\")\n",
        "\n",
        "    return gbest_position, gbest_score"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "14367e62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verbose worker implementation with clear prints\n",
        "\n",
        "\n",
        "def worker_node(worker_id, cmd_queue, result_queue, split_index,\n",
        "                base_lr=1e-3, local_epochs=5):\n",
        "    \"\"\"Worker loop running in a separate process (verbose).\n",
        "\n",
        "    Each worker:\n",
        "    - Receives the current global model parameters.\n",
        "    - Trains locally with Adam for a few epochs on its data split.\n",
        "    - Sends back its best local model and metrics.\n",
        "    \"\"\"\n",
        "    print(f\"[Worker {worker_id}] Starting. Using split index {split_index}.\")\n",
        "\n",
        "    # Recreate model and data inside the process (safe under spawn)\n",
        "    model = create_model()\n",
        "    loaders = split_loaders[split_index]\n",
        "    train_loader = loaders[\"train_loader\"]\n",
        "    test_loader = loaders[\"test_loader\"]\n",
        "\n",
        "    while True:\n",
        "        msg = cmd_queue.get()  # 1 read on cmd_queue\n",
        "        msg_type = msg.get(\"type\", None)\n",
        "        round_idx = msg.get(\"round\", \"-\")\n",
        "\n",
        "        if msg_type == \"stop\":\n",
        "            print(f\"[Worker {worker_id}] Received stop signal. Exiting.\")\n",
        "            break\n",
        "\n",
        "        assert msg_type == \"train\", f\"Unknown message type: {msg_type}\"\n",
        "        print(f\"[Worker {worker_id}] Received TRAIN command for round {round_idx}.\")\n",
        "\n",
        "        global_state = msg[\"state_dict\"]\n",
        "\n",
        "        # Load global model\n",
        "        model.load_state_dict(global_state)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=base_lr)\n",
        "\n",
        "        # Local training (Adam)\n",
        "        last_train_loss = None\n",
        "        for local_ep in range(local_epochs):\n",
        "            last_train_loss = train_one_epoch_adam(model, train_loader, optimizer, loss_fn)\n",
        "            print(f\"[Worker {worker_id}] Round {round_idx} | Local epoch {local_ep+1}/{local_epochs} | train_loss={last_train_loss:.6f}\")\n",
        "\n",
        "        # Evaluate on local test data\n",
        "        local_test_loss = evaluate_model(model, test_loader, loss_fn)\n",
        "        print(f\"[Worker {worker_id}] Round {round_idx} | Finished local training. test_loss={local_test_loss:.6f}\")\n",
        "\n",
        "        # Send result back to main node (1 write on result_queue)\n",
        "        result_queue.put({\n",
        "            \"worker_id\": worker_id,\n",
        "            \"round\": round_idx,\n",
        "            \"state_dict\": copy.deepcopy(model.state_dict()),\n",
        "            \"train_loss\": float(last_train_loss),\n",
        "            \"test_loss\": float(local_test_loss),\n",
        "        })\n",
        "        print(f\"[Worker {worker_id}] Round {round_idx} | Result sent back to main.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a5ac0e8a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verbose main orchestration so we can follow progress clearly\n",
        "\n",
        "\n",
        "def run_hybrid_pso_adam(num_rounds=3,\n",
        "                         num_workers=5,\n",
        "                         local_epochs=5,\n",
        "                         global_adam_epochs=3,\n",
        "                         base_lr=1e-3):\n",
        "    assert num_workers <= len(split_loaders)\n",
        "\n",
        "    print(\"\\n[Main] =========================================\")\n",
        "    print(\"[Main] Starting hybrid PSO–Adam run\")\n",
        "    print(f\"[Main] num_rounds={num_rounds}, num_workers={num_workers}, local_epochs={local_epochs}, global_adam_epochs={global_adam_epochs}, base_lr={base_lr}\")\n",
        "    print(\"[Main] =========================================\\n\")\n",
        "\n",
        "    # Queues for IPC\n",
        "    cmd_queues = [Queue() for _ in range(num_workers)]  # per-worker command queue\n",
        "    result_queue = Queue()  # shared results queue\n",
        "\n",
        "    # Spawn worker processes\n",
        "    workers = []\n",
        "    for wid in range(num_workers):\n",
        "        print(f\"[Main] Spawning worker process {wid} for split {wid}.\")\n",
        "        p = Process(\n",
        "            target=worker_node,\n",
        "            args=(wid, cmd_queues[wid], result_queue, wid, base_lr, local_epochs),\n",
        "        )\n",
        "        p.start()\n",
        "        workers.append(p)\n",
        "\n",
        "    print(\"[Main] All workers spawned. Entering communication rounds.\\n\")\n",
        "\n",
        "    # Main node model\n",
        "    global_model = create_model()\n",
        "\n",
        "    # Use the first split's test loader as a simple global validation set\n",
        "    val_loader = split_loaders[0][\"test_loader\"]\n",
        "\n",
        "    history = {\n",
        "        \"round\": [],\n",
        "        \"global_val_loss\": [],\n",
        "        \"avg_worker_test_loss\": [],\n",
        "    }\n",
        "\n",
        "    reads_per_round = []\n",
        "    writes_per_round = []\n",
        "\n",
        "    for r in range(num_rounds):\n",
        "        print(f\"\\n[Main] ===== Communication Round {r} =====\")\n",
        "\n",
        "        # 1) Main node broadcasts current global model to all workers\n",
        "        state_to_send = copy.deepcopy(global_model.state_dict())\n",
        "        print(f\"[Main] Broadcasting global model to {num_workers} workers...\")\n",
        "        for q in cmd_queues:\n",
        "            q.put({\"type\": \"train\", \"round\": r, \"state_dict\": state_to_send})\n",
        "        writes_this_round = num_workers  # command writes\n",
        "        print(f\"[Main] Broadcast complete. (writes_this_round={writes_this_round})\")\n",
        "\n",
        "        # 2) Collect results from workers\n",
        "        worker_states = []\n",
        "        worker_test_losses = []\n",
        "        print(\"[Main] Waiting for worker results...\")\n",
        "        for _ in range(num_workers):\n",
        "            msg = result_queue.get()\n",
        "            wid = msg[\"worker_id\"]\n",
        "            worker_states.append(msg[\"state_dict\"])\n",
        "            worker_test_losses.append(msg[\"test_loss\"])\n",
        "            print(\n",
        "                f\"[Main] Received result from worker {wid} | \"\n",
        "                f\"round={msg['round']} | train_loss={msg['train_loss']:.6f} | test_loss={msg['test_loss']:.6f}\"\n",
        "            )\n",
        "        reads_this_round = num_workers  # result reads\n",
        "        print(f\"[Main] Collected all worker results. (reads_this_round={reads_this_round})\")\n",
        "\n",
        "        # 3) PSO aggregation on the worker models (hybrid PSO + Adam)\n",
        "        print(\"[Main] Starting PSO-based aggregation over worker models...\")\n",
        "        model_template = create_model()\n",
        "        best_weights, best_score = pso_optimize_aggregation(\n",
        "            worker_states,\n",
        "            val_loader,\n",
        "            model_template,\n",
        "            loss_fn,\n",
        "            num_particles=10,\n",
        "            max_iters=5,\n",
        "        )\n",
        "        agg_state = aggregate_models_cpu(best_weights, worker_states)\n",
        "        print(f\"[Main] PSO aggregation done. Best validation loss from PSO={best_score:.6f}\")\n",
        "\n",
        "        # 4) Main node runs Adam on aggregated model for a few epochs\n",
        "        global_model.load_state_dict(agg_state)\n",
        "        optimizer = torch.optim.Adam(global_model.parameters(), lr=base_lr)\n",
        "        print(f\"[Main] Running Adam on aggregated model for {global_adam_epochs} epochs...\")\n",
        "        for ge in range(global_adam_epochs):\n",
        "            train_loss = train_one_epoch_adam(global_model, val_loader, optimizer, loss_fn)\n",
        "            print(f\"[Main] Global Adam epoch {ge+1}/{global_adam_epochs} | train_loss={train_loss:.6f}\")\n",
        "\n",
        "        # 5) Log metrics\n",
        "        global_val_loss = evaluate_model(global_model, val_loader, loss_fn)\n",
        "        history[\"round\"].append(r)\n",
        "        history[\"global_val_loss\"].append(float(global_val_loss))\n",
        "        history[\"avg_worker_test_loss\"].append(float(np.mean(worker_test_losses)))\n",
        "\n",
        "        reads_per_round.append(reads_this_round)\n",
        "        writes_per_round.append(writes_this_round)\n",
        "\n",
        "        print(\n",
        "            f\"[Main] Round {r} summary | global_val_loss={global_val_loss:.6f}, \"\n",
        "            f\"avg_worker_test_loss={np.mean(worker_test_losses):.6f}, \"\n",
        "            f\"writes={writes_this_round}, reads={reads_this_round}\"\n",
        "        )\n",
        "\n",
        "    # 6) Stop workers (one extra command per worker)\n",
        "    print(\"\\n[Main] All communication rounds complete. Sending stop signals to workers...\")\n",
        "    for q in cmd_queues:\n",
        "        q.put({\"type\": \"stop\"})\n",
        "    extra_writes = num_workers\n",
        "    print(f\"[Main] Stop messages sent. (extra_writes={extra_writes})\")\n",
        "\n",
        "    for p in workers:\n",
        "        p.join()\n",
        "    print(\"[Main] All workers have terminated.\")\n",
        "\n",
        "    # Analytical IPC accounting (Queue-based, each message = 1 write + 1 read)\n",
        "    train_messages = num_rounds * (2 * num_workers)  # train commands + results\n",
        "    stop_messages = num_workers  # stop commands\n",
        "    total_messages = train_messages + stop_messages\n",
        "\n",
        "    comm_stats = {\n",
        "        \"num_rounds\": num_rounds,\n",
        "        \"num_workers\": num_workers,\n",
        "        \"reads_per_round\": reads_per_round,\n",
        "        \"writes_per_round\": writes_per_round,\n",
        "        \"total_train_round_messages\": train_messages,\n",
        "        \"total_stop_messages\": stop_messages,\n",
        "        \"total_queue_writes\": total_messages,\n",
        "        \"total_queue_reads\": total_messages,\n",
        "    }\n",
        "\n",
        "    print(\"\\n[Main] Run complete. Communication statistics:\")\n",
        "    for k, v in comm_stats.items():\n",
        "        print(f\"[Main]   {k}: {v}\")\n",
        "\n",
        "    return global_model, history, comm_stats"
      ],
      "id": "6c80775e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple PSO over aggregation weights (reusing the idea from Approach_1)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def pso_optimize_aggregation(node_states, val_loader, model_template, loss_fn,\n",
        "                             num_particles=10, max_iters=5, w=0.7, c1=1.5, c2=1.5):\n",
        "    \"\"\"Small PSO to find good aggregation weights for the 5 workers.\"\"\"\n",
        "    num_nodes = len(node_states)\n",
        "\n",
        "    # Initialize particles on the simplex via Dirichlet\n",
        "    particles = np.random.dirichlet(np.ones(num_nodes), size=num_particles)\n",
        "    velocities = np.zeros_like(particles)\n",
        "    pbest_positions = particles.copy()\n",
        "    pbest_scores = np.full(num_particles, np.inf)\n",
        "\n",
        "    gbest_position = None\n",
        "    gbest_score = np.inf\n",
        "\n",
        "    for it in range(max_iters):\n",
        "        for i in range(num_particles):\n",
        "            w_vec = np.abs(particles[i])\n",
        "            w_vec /= np.sum(w_vec)\n",
        "\n",
        "            agg_state = aggregate_models_cpu(w_vec, node_states)\n",
        "            score = evaluate_model_state(agg_state, model_template, val_loader, loss_fn)\n",
        "\n",
        "            if score < pbest_scores[i]:\n",
        "                pbest_scores[i] = score\n",
        "                pbest_positions[i] = w_vec.copy()\n",
        "\n",
        "            if score < gbest_score:\n",
        "                gbest_score = score\n",
        "                gbest_position = w_vec.copy()\n",
        "\n",
        "        # Velocity/position update\n",
        "        for i in range(num_particles):\n",
        "            r1 = np.random.rand(num_nodes)\n",
        "            r2 = np.random.rand(num_nodes)\n",
        "            velocities[i] = (\n",
        "                w * velocities[i]\n",
        "                + c1 * r1 * (pbest_positions[i] - particles[i])\n",
        "                + c2 * r2 * (gbest_position - particles[i])\n",
        "            )\n",
        "            particles[i] += velocities[i]\n",
        "\n",
        "    return gbest_position, gbest_score"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a15b68a1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "GDP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}